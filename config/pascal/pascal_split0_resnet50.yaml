DATA:
  data_root: ./VOCdevkit/VOC2012
  train_list: /lists/pascal/voc_sbd_merge_noduplicate.txt
  val_list: ./lists/val.txt
  classes: 2


TRAIN:
  layers: 50
  sync_bn: False
  train_h: 473
  train_w: 473
  val_size: 473
  scale_min: 0.9  # minimum random scale
  scale_max: 1.1 # maximum random scale
  rotate_min: -10  # minimum random rotate
  rotate_max: 10  # maximum random rotate
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]
  ignore_label: 255
  padding_label: 255
  aux_weight: 1.0
  train_gpu: [0]
  workers: 8  # data loader workers
  batch_size: 4  # batch size for training
  batch_size_val: 1
  base_lr: 0.0025
  epochs: 100
  start_epoch: 0
  power: 0.9 # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed: 321
  print_freq: 5
  save_freq: 20
  save_path: exp/pascal/split0_resnet50/model
  weight:  # ./exp/pascal/split0_resnet50/model/train_epoch_80_0.5743127998643639.pth
  resume:  # path to latest checkpoint (default: none)
  evaluate: True
  split: 3
  shot: 1 # 1/5
  vgg: False
  ppm_scales: [60, 30, 15, 8]
  fix_random_seed_val: True
  warmup: False
  use_coco: False
  use_split_coco: False
  resized_val: True
  ori_resize: True  # use original label for evaluation
  schedule_sampler: 'uniform'
  lr_anneal_steps: 0
  microbatch: -1  # -1 disables microbatches
  ema_rate: '0.9999'  # comma-separated list of EMA values
  log_interval: 100
  save_interval: 5000
  resume_checkpoint: ''
  use_fp16: False
  fp16_scale_growth: 1e-3
  clip_denoised: True
  num_samples: 1
  model_path: results3/savedmodel001277.pt # result0,1,2,3 to fold0,1,2,3
  image_size: 60
  num_channels: 128
  class_cond: False
  num_res_blocks: 2
  num_heads: 1
  learn_sigma: True
  use_scale_shift_norm: False
  attention_resolutions: '16'
  diffusion_steps: 100
  noise_schedule: linear
  rescale_learned_sigmas: False
  rescale_timesteps: False
  num_head_channels: -1
  num_heads_upsample: -1
  channel_mult: ''
  dropout: 0
  use_checkpoint: False
  resblock_updown: False
  use_new_attention_order: False
  timestep_respacing: ''
  use_kl: False
  predict_xstart: False

## deprecated multi-processing training
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0
  use_apex: False
  opt_level: 'O0'
  keep_batchnorm_fp32:
  loss_scale:

